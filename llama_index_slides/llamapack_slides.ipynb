{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ELGBYVAj_7W9"
      },
      "outputs": [],
      "source": [
        "# !pip install llama-hub\n",
        "# !pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6rz061rqA2Kv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "M3Hp6jdBIHb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex, download_loader\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "from pathlib import Path\n"
      ],
      "metadata": {
        "id": "2Pk8IeE_EL1M"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLamaHub"
      ],
      "metadata": {
        "id": "A3Kq-YvRIRKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PptxReader = download_loader(\"PptxReader\")\n",
        "loader = PptxReader()\n",
        "documents = loader.load_data(file=Path('/content/Explainable AI.pptx'))"
      ],
      "metadata": {
        "id": "gHeqTz6hAbQO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "tkrCSv8VBO46"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "openai.api_key = \"sk-xxxxxx\""
      ],
      "metadata": {
        "id": "i37um9arBZ3j"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parsing Data in slides as documents"
      ],
      "metadata": {
        "id": "g-ua5vfKIaVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA31j6HDAbio",
        "outputId": "aa5c7c13-a4bb-42b9-8fd5-aa4dcdfad118"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "gCroCUvGH4_C"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"what is explainable ai\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuCuJrQbH7Q0",
        "outputId": "a55205ed-bab1-4ee4-ae74-f739f993b074"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explainable AI is an emerging field in machine learning that focuses on understanding and explaining the decision-making process of AI systems. It aims to address the \"black box\" nature of AI by inspecting and analyzing the steps and models involved in making decisions. Explainable AI seeks to answer questions such as why a specific prediction or decision was made, why alternative actions were not taken, and how the AI system can correct errors. Researchers have been developing tools and techniques, such as the What-if Tool, LIME, and TreeInterpreter, to promote explainable AI. The goal is to provide justifications and explanations for AI decisions, increasing trust and understanding for analysts and consumers.\n"
          ]
        }
      ]
    }
  ]
}