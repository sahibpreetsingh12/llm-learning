{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6cbc11",
   "metadata": {},
   "source": [
    "## Installations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85076393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-groq\n",
    "# %pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai langchain-chroma bs4\n",
    "# !pip install shutup\n",
    "# !pip install sentence-transformers==2.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c21e9a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a901409",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutup\n",
    "import torch\n",
    "from typing import List, Dict, Tuple\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from torch import Tensor\n",
    "from langchain_groq import ChatGroq\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers.readers import InputExample\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.losses import CoSENTLoss, MatryoshkaLoss\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c98fedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "loader = TextLoader(\"essay.txt\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebc8b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split text into chunks \n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df1ca8",
   "metadata": {},
   "source": [
    "## GROQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "305969cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_key = 'yourgroq-key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f98b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "matryoshka_dim = 64\n",
    "\n",
    "# Define the embedding model\n",
    "#using mixbread's embedding and in binary mode with truncated (matryoshka embeddings)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"mixedbread-ai/mxbai-embed-large-v1\", \n",
    "                                   model_kwargs = {'truncate_dim':matryoshka_dim},\n",
    "                                   encode_kwargs = {'precision': 'binary'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1582683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS the vector store \n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Define a retriever interface\n",
    "retriever = vector.as_retriever()\n",
    "\n",
    "# Define LLM\n",
    "model = ChatGroq(temperature=0, groq_api_key=groq_key, model_name=\"mixtral-8x7b-32768\")\n",
    "\n",
    "# Define prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "# Create a retrieval chain to answer questions\n",
    "document_chain = create_stuff_documents_chain(model, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd21df45",
   "metadata": {},
   "source": [
    "## retrieved responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e12ae09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2022 FIFA World Cup was held in Qatar, making it the first World Cup to be held in the Arab world and Muslim world, and the second held entirely in Asia after the 2002 tournament in South Korea and Japan.\n",
      "CPU times: user 150 ms, sys: 67.4 ms, total: 217 ms\n",
      "Wall time: 1.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = retrieval_chain.invoke({\"input\": \"where did fifa happened ?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7251007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
